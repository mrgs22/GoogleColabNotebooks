{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMJ5uGyiMjqm8wFHMOh55Te"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import gensim\n","from gensim import corpora\n","from gensim.models import LdaModel\n","from gensim.models import CoherenceModel"],"metadata":{"id":"CqkebPMKfsZ2","executionInfo":{"status":"ok","timestamp":1713806757426,"user_tz":240,"elapsed":179,"user":{"displayName":"Gregory Sylvester","userId":"05010306304674874100"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","execution_count":44,"metadata":{"id":"-KqieZreyBsl","executionInfo":{"status":"ok","timestamp":1713808607214,"user_tz":240,"elapsed":148,"user":{"displayName":"Gregory Sylvester","userId":"05010306304674874100"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"1fa62c70-b71e-4870-d129-395f37daa3f6"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":44}],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from bs4 import BeautifulSoup #all data collection i got help from https://www.youtube.com/watch?v=yME299lFvFk\n","import requests\n","import nltk\n","import string\n","from tqdm import tqdm\n","from sklearn.decomposition import LatentDirichletAllocation\n","from sklearn.feature_extraction.text import CountVectorizer\n","nltk.download('stopwords')\n","nltk.download('punkt')"]},{"cell_type":"code","source":["stemmer = nltk.stem.PorterStemmer()\n","stopwords = nltk.corpus.stopwords.words(\"english\")\n","vectorizer = CountVectorizer()\n","vocab = []\n","vocab_counts = []\n","vocab_dic = 0"],"metadata":{"id":"y8_JDRX30JhD","executionInfo":{"status":"ok","timestamp":1713806763290,"user_tz":240,"elapsed":2,"user":{"displayName":"Gregory Sylvester","userId":"05010306304674874100"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["Agriculture_urls = [\n","    \"https://www.govinfo.gov/content/pkg/CHRG-118hhrg52370/html/CHRG-118hhrg52370.htm\",\n","    \"https://www.govinfo.gov/content/pkg/CHRG-118hhrg52205/html/CHRG-118hhrg52205.htm\",\n","    \"https://www.govinfo.gov/content/pkg/CHRG-118hhrg52557/html/CHRG-118hhrg52557.htm\",\n","    \"https://www.govinfo.gov/content/pkg/CHRG-118hhrg52371/html/CHRG-118hhrg52371.htm\",\n","    \"https://www.govinfo.gov/content/pkg/CHRG-118hhrg54212/html/CHRG-118hhrg54212.htm\"\n","]\n","\n","Armed_services_urls = [\n","    \"https://www.govinfo.gov/content/pkg/CHRG-118hhrg51700/html/CHRG-118hhrg51700.htm\",\n","    \"https://www.govinfo.gov/content/pkg/CHRG-118hhrg51704/html/CHRG-118hhrg51704.htm\",\n","    \"https://www.govinfo.gov/content/pkg/CHRG-118hhrg51705/html/CHRG-118hhrg51705.htm\",\n","    \"https://www.govinfo.gov/content/pkg/CHRG-118hhrg53350/html/CHRG-118hhrg53350.htm\",\n","    \"https://www.govinfo.gov/content/pkg/CHRG-118hhrg52380/html/CHRG-118hhrg52380.htm\"\n","]\n"],"metadata":{"id":"jUxG3KcByIVw","executionInfo":{"status":"ok","timestamp":1713806763510,"user_tz":240,"elapsed":221,"user":{"displayName":"Gregory Sylvester","userId":"05010306304674874100"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["My dataset consists of each paragraph in each of the hearings as documents."],"metadata":{"id":"ajMNSYTgzup6"}},{"cell_type":"code","source":["#this method I got help from chatgpt to pull the txt from the wepages\n","def url_to_txt (url):\n","  response = requests.get(url)\n","  soup = BeautifulSoup(response.text, 'html.parser')\n","  body = soup.find('body')\n","  text = body.get_text(separator='\\n')\n","  return text\n","\n","def text_splitter(doc):\n","  dump = []\n","  doc = str(doc)\n","  docs = doc.split(\"    \")\n","  for doc_len in docs:\n","    if len(doc_len) >= 150:\n","      dump.append(doc_len)\n","  print(len(dump))\n","  return dump\n","\n","#https://spotintelligence.com/2022/12/21/nltk-preprocessing-pipeline/\n","def preprocessing_doc(docs):\n","  dump = []\n","  for i in range(len(docs)):\n","    for j in range(len(docs[i])):\n","      tokens = nltk.word_tokenize(str(docs[i][j]))\n","      tokens = [token.lower() for token in tokens]\n","      tokens = [token for token in tokens if token not in string.punctuation]\n","      tokens = [token for token in tokens if token.lower() not in stopwords]\n","      tokens = [token for token in tokens if not any(char.isdigit() for char in token)]#this line i got help for checking the entire stirng for any digits\n","      stemmed_tokens = [stemmer.stem(token) for token in tokens]\n","      docs[i][j] = stemmed_tokens\n","      dump.append(docs[i][j])\n","  return dump\n","\n","\n","\n","\n","def pad_sequence(sequence, max_length):\n","    return sequence + [0] * (max_length - len(sequence))\n","\n","def bag_of_words(docs_1, docs_2):\n","  total_len = len(docs_1) + len(docs_2)\n","  max_length = max(len(doc) for doc in docs_1 + docs_2)\n","\n","  print(total_len, len(docs_1), len(docs_2))\n","  padded_docs_1 = [pad_sequence(doc, max_length) for doc in docs_1]\n","  padded_docs_2 = [pad_sequence(doc, max_length) for doc in docs_2]\n","\n","\n","  #I was haveing issues with the input size for using the np libray this following line and padded_seq funciton i got help from chatgpt to fix bug with not same dimensions for vocab def\n","  docs_np = np.concatenate((padded_docs_1, padded_docs_2))\n","\n","  vocab, vocab_counts = np.unique(docs_np.flatten(), return_counts=True)\n","  vocab_dic = {vocab[x]: x for x in range(len(vocab))}\n","  bag_of_words = np.zeros((total_len, len(vocab)))\n","\n","\n","  for i in tqdm(range(len(padded_docs_1))):\n","      for token in padded_docs_1[i]:\n","        if token != 0:\n","          bag_of_words[i][vocab_dic[token]] += 1\n","\n","  for i in tqdm(range(len(padded_docs_2))):\n","      for token in padded_docs_2[i]:\n","        if token != 0:\n","          bag_of_words[i + len(padded_docs_1)][vocab_dic[token]] += 1\n","  print(bag_of_words.shape)\n","  return bag_of_words, vocab, vocab_counts,vocab_dic\n","\n","def naive_bayes(bag_of_words, vocab, vector='count', count_min=15):\n","    # Filter out words with counts below the threshold\n","    docs_1 = bag_of_words[:1712]\n","    docs_2 = bag_of_words[1712:]\n","\n","    counts_docs_1 = np.sum(docs_1, axis=0)\n","    counts_docs_2 = np.sum(docs_2, axis=0)\n","\n","    # Filter out words with counts below count_min\n","    index_doc_1 = np.where(counts_docs_1 > count_min)[0]\n","    index_doc_2 = np.where(counts_docs_2 > count_min)[0]\n","\n","    # Compute probabilities using filtered counts\n","    prob_1 = np.log(np.divide(counts_docs_1[index_doc_1], counts_docs_2[index_doc_1] + 0.001))\n","    prob_2 = np.log(np.divide(counts_docs_2[index_doc_2], counts_docs_1[index_doc_2] + 0.001))\n","\n","    # Select top 50 words\n","    doc_1_10 = np.argsort(prob_1)[-50:]\n","    doc_2_10 = np.argsort(prob_2)[-50:]\n","\n","    docs_1_10_w = vocab[doc_1_10]\n","    docs_2_10_w = vocab[doc_2_10]\n","\n","    return docs_1_10_w, docs_2_10_w\n"],"metadata":{"id":"2ut3Os8Nzt6R","executionInfo":{"status":"ok","timestamp":1713806763510,"user_tz":240,"elapsed":1,"user":{"displayName":"Gregory Sylvester","userId":"05010306304674874100"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["agr_docs = np.array([url_to_txt(doc) for doc in Agriculture_urls])\n","as_docs = np.array([url_to_txt(doc) for doc in Armed_services_urls])\n"],"metadata":{"id":"wOEuNSa09fWM","executionInfo":{"status":"ok","timestamp":1713806769945,"user_tz":240,"elapsed":6302,"user":{"displayName":"Gregory Sylvester","userId":"05010306304674874100"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["new_agr_docs = []\n","for doc in agr_docs:\n","    new_agr_docs.append(text_splitter(doc))\n","agr_docs = new_agr_docs\n","\n","# Process documents in as_docs\n","new_as_docs = []\n","for doc in as_docs:\n","    new_as_docs.append(text_splitter(doc))\n","as_docs = new_as_docs"],"metadata":{"id":"Q3PiqpJ9AN6Z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713806770126,"user_tz":240,"elapsed":190,"user":{"displayName":"Gregory Sylvester","userId":"05010306304674874100"}},"outputId":"8ae1f621-6623-4acb-8bf9-4d4f7279a58a"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["1591\n","352\n","560\n","243\n","866\n","64\n","503\n","658\n","156\n","331\n"]}]},{"cell_type":"code","source":["agr_docs = preprocessing_doc(agr_docs)\n","as_docs = preprocessing_doc(as_docs)"],"metadata":{"id":"6WR9SJdvlPCd","executionInfo":{"status":"ok","timestamp":1713806782889,"user_tz":240,"elapsed":12764,"user":{"displayName":"Gregory Sylvester","userId":"05010306304674874100"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["bow, vocab,vocab_counts,vocab_dic =  bag_of_words(as_docs,agr_docs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DE4qfYSQFUfR","executionInfo":{"status":"ok","timestamp":1713806787201,"user_tz":240,"elapsed":4351,"user":{"displayName":"Gregory Sylvester","userId":"05010306304674874100"}},"outputId":"0f092974-6d76-490e-ef83-b3df9b825eec"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["5324 1712 3612\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1712/1712 [00:00<00:00, 7816.96it/s]\n","100%|██████████| 3612/3612 [00:00<00:00, 6004.05it/s]\n"]},{"output_type":"stream","name":"stdout","text":["(5324, 9210)\n"]}]},{"cell_type":"code","source":["w_1, w_2 = naive_bayes(bow, vocab)\n","print(w_1, w_2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-A2D2G7bPLdW","executionInfo":{"status":"ok","timestamp":1713806787371,"user_tz":240,"elapsed":172,"user":{"displayName":"Gregory Sylvester","userId":"05010306304674874100"}},"outputId":"d7271dd3-9001-4e57-a651-76934845ee65"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["['afford' 'aia' 'all-in' 'amount'\n"," '.................................................' 'advantag'\n"," 'all-of-the-abov' 'almost' 'anchor' 'arrest'\n"," '//www.ams.usda.gov/reports/agricultural-competition-' 'asylum'\n"," 'acquisit' 'airborn' 'ambul' 'apa' '//www.ers.usda.gov/data-' 'amplif'\n"," '//www.fsis.usda.gov/shared/pdf/' 'action-ori' 'ag-'\n"," '//www.fsis.usda.gov/news-' 'amort' 'approach' 'alon' 'apart' 'acquir'\n"," '....................................................' 'alli' 'attorney'\n"," 'almond' 'agra' '//www.nationalchickencouncil.org/' 'along' 'ams-' 'acid'\n"," 'among' 'arab' 'altitud' 'apprentic' 'allud' 'acep'\n"," '//www.ers.usda.gov/topics/farm-economy/farm-sector-' 'alter'\n"," 'apprenticeship' 'astorga' 'aquat' 'across' 'age/sex'\n"," '.......................................'] ['basket' 'aviagen' 'chinese-own' 'asylum' '//democrats-' 'backyard'\n"," 'andrew' 'ala.' 'cline' 'civil' 'aris' 'autom' 'badli' 'brown' 'barri'\n"," 'agrosecur' 'brave' 'alpha'\n"," '//www.aei.org/research-products/report/subsidized-beef-' 'american'\n"," 'around' 'background' 'big-pictur' 'belaru' 'alaska'\n"," '//www.esma.europa.eu/sites/default/' 'buffet' 'cohen' 'adverb' 'absorb'\n"," 'capital-intens' 'accordingli' 'blue-wat' 'adel' 'april' 'aye' 'carbaj'\n"," 'agrochem' 'black-ey' 'cme'\n"," '//www.ers.usda.gov/data-products/ag-and-food-' 'bureau' 'aquat'\n"," 'artifici' 'airplan' 'bridg' 'asymptomat' 'aros' 'adjust' 'climate-rel']\n"]}]},{"cell_type":"markdown","source":["# 2.4 Topic Modeling\n","\n","\n","---\n","\n","\n","Run Latent Dirichlet Allocation using all of the documents in your corpus. You\n","may choose the number of topics that you feel is most appropriate and gives\n","the results that either look most reasonable to you or optimize a metric like\n","coherence. You do not need to implement LDA yourself, and should use a\n","3rd-party library like gensim or mallet (or any other reputable library you find\n","for LDA topic modeling) to do the topic modeling and may use libraries like\n","PyLDAvis to help present your results. You may also use jsLDA for both topic\n","modeling and generating some interesting visualizations (but you may not use\n","the example corpus presented there).\n","You should present your topics (or a selection of the topics you think are most\n","interesting/useful) in a table where the first column contains your own manually\n","3\n","assigned label for the topic (e.g., ”school”), and the subsequent columns contain\n","a the top 10 (or more) terms from that topic, sorted by their probability of\n","belonging to that topic, along with their probabilities (e.g. ”homework” (0.02),\n","”class” (0.01), and so on).\n","Finally, for each category, find the average distribution of all topics for documents\n","in that category and report the top 3-5 topics for each category. You\n","can determine this by taking the topic distribution for each document in a given\n","category and averaging the probabilities for each topic across all documents in\n","the category."],"metadata":{"id":"CXWEtbNWvhEm"}},{"cell_type":"code","source":["print(bow.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"79ZJMeq628ib","executionInfo":{"status":"ok","timestamp":1713807035616,"user_tz":240,"elapsed":106,"user":{"displayName":"Gregory Sylvester","userId":"05010306304674874100"}},"outputId":"8585941f-84c0-4f27-c379-a6c69b5b0787"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["(5324, 9210)\n"]}]},{"cell_type":"code","source":["\n","\n","combined_docs = agr_docs + as_docs\n","\n","text_documents = [' '.join(doc) for doc in combined_docs]\n","\n","vectorizer = CountVectorizer()\n","X = vectorizer.fit_transform(text_documents)\n","num_topics = 5\n","lda_model = LatentDirichletAllocation(n_components=num_topics, random_state=22)\n","lda_model.fit(X)\n","\n"],"metadata":{"id":"tYFYi4cDe80k","colab":{"base_uri":"https://localhost:8080/","height":75},"executionInfo":{"status":"ok","timestamp":1713808639985,"user_tz":240,"elapsed":22153,"user":{"displayName":"Gregory Sylvester","userId":"05010306304674874100"}},"outputId":"1c311b2f-4457-45c8-eb96-9d0e0cd7d285"},"execution_count":45,"outputs":[{"output_type":"execute_result","data":{"text/plain":["LatentDirichletAllocation(n_components=5, random_state=22)"],"text/html":["<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LatentDirichletAllocation(n_components=5, random_state=22)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LatentDirichletAllocation</label><div class=\"sk-toggleable__content\"><pre>LatentDirichletAllocation(n_components=5, random_state=22)</pre></div></div></div></div></div>"]},"metadata":{},"execution_count":45}]},{"cell_type":"code","source":["def display_topics(model, vectorizer, no_top_words):\n","    feature_names = vectorizer.get_feature_names_out()\n","    for topic_idx, topic in enumerate(model.components_):\n","        print(f\"Topic {topic_idx}:\")\n","        topic /= topic.sum()\n","        top_indices = topic.argsort()[:-no_top_words - 1:-1]\n","        top_terms = [feature_names[i] for i in top_indices]\n","        top_probabilities = [topic[i] for i in top_indices]\n","        for term, prob in zip(top_terms, top_probabilities):\n","            if not term.isdigit():\n","                print(f\"    {term} ({prob:.4f})\")\n","\n","no_top_words = 10\n","print(\"Topics and top words with probabilities:\\n\")\n","display_topics(lda_model, vectorizer, no_top_words)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3y0vGQZL4UzK","executionInfo":{"status":"ok","timestamp":1713808639985,"user_tz":240,"elapsed":2,"user":{"displayName":"Gregory Sylvester","userId":"05010306304674874100"}},"outputId":"00aa8267-128a-4b32-cd69-20541c31bc44"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["Topics and top words with probabilities:\n","\n","Topic 0:\n","    agricultur (0.0138)\n","    food (0.0123)\n","    farmer (0.0122)\n","    usda (0.0116)\n","    farm (0.0104)\n","    program (0.0092)\n","    product (0.0092)\n","    produc (0.0086)\n","    increas (0.0074)\n","    research (0.0070)\n","Topic 1:\n","    forest (0.0193)\n","    servic (0.0100)\n","    program (0.0098)\n","    manag (0.0093)\n","    land (0.0093)\n","    state (0.0091)\n","    need (0.0087)\n","    author (0.0083)\n","    feder (0.0080)\n","    work (0.0075)\n","Topic 2:\n","    think (0.0151)\n","    go (0.0096)\n","    china (0.0091)\n","    defens (0.0088)\n","    would (0.0086)\n","    need (0.0085)\n","    know (0.0082)\n","    mr (0.0077)\n","    thing (0.0073)\n","    year (0.0069)\n","Topic 3:\n","    mr (0.0220)\n","    thank (0.0134)\n","    think (0.0118)\n","    work (0.0101)\n","    go (0.0100)\n","    would (0.0100)\n","    farm (0.0087)\n","    member (0.0086)\n","    make (0.0084)\n","    committe (0.0084)\n","Topic 4:\n","    market (0.0224)\n","    would (0.0120)\n","    product (0.0115)\n","    propos (0.0110)\n","    risk (0.0107)\n","    grower (0.0087)\n","    contract (0.0076)\n","    requir (0.0075)\n","    chicken (0.0075)\n","    price (0.0070)\n"]}]}]}